<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://dandelionym.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dandelionym.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-19T05:36:49+00:00</updated><id>https://dandelionym.github.io/feed.xml</id><title type="html">Mellen Y. Pu</title><subtitle>Mellen&apos;s personal website. </subtitle><entry><title type="html">Can Language Models “Understand” the World? A Misleading Question</title><link href="https://dandelionym.github.io/blog/2024/can-language-models-understand-the-world-a-misleading-question/" rel="alternate" type="text/html" title="Can Language Models “Understand” the World? A Misleading Question"/><published>2024-08-22T05:24:04+00:00</published><updated>2024-08-22T05:24:04+00:00</updated><id>https://dandelionym.github.io/blog/2024/can-language-models-understand-the-world-a-misleading-question</id><content type="html" xml:base="https://dandelionym.github.io/blog/2024/can-language-models-understand-the-world-a-misleading-question/"><![CDATA[<p>The concept of “<strong>understanding</strong>” is inherently subjective, especially when evaluating GPT-series models, which are often considered black boxes. Different people may hold varying perspectives on what constitutes understanding. <strong>While some argue that GPT-4 lacks true comprehension, it’s worth questioning how we measure understanding in humans.</strong> What level of performance or test scores can definitively distinguish between a solid grasp of concepts and mere conjecture?</p> <p>I contend that framing the debate around whether these models “understand” is misleading.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GZ-CbE5OqMdWw9Pt"/><figcaption>Photo by <a href="https://unsplash.com/@benwhitephotography?utm_source=medium&amp;utm_medium=referral">Ben White</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>Let’s start by discussing the <strong>reasoning process</strong>. The most effective approach is to view it from the perspective of general learning. This process is analogous to how humans acquire knowledge through reading, frequent reflection, memorization, and interaction.</p> <p>The reasoning ability of language models can be understood as a reflection of rationality in cognition, or as a data-centric learning problem, rooted in continuous learning from large-scale datasets. Within these human-annotated datasets, cognitive logic and causality from a human perspective are evident. The design of these models aims to enable them to learn both implicit and explicit correlations from token-level representations (commonly understood as word-level, though it’s more nuanced than just individual words). The process of learning natural language tokens is akin to learning images at the pixel level. For instance, just as one can generate images by understanding the distribution of pixels (e.g., mouths are located at the bottom of faces, symmetrical eyes, etc.), in language processing, a “pixel” corresponds to a token. The distribution of a sentence (comprising multiple tokens) conveys information in a way that mirrors human recognition of patterns and characteristics.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YQgo0ViHx9cpgJ2v.png"/><figcaption>Features or representations provided by a good DNN model should encode meaning related to the current task (in this case, classification). With this representation, similar data items are <em>closer</em> to each other (right side); the original non-linear problem is now linearly separable, and therefore easier to solve. (referenced from <a href="https://blog.fastforwardlabs.com/2020/11/15/representation-learning-101-for-software-engineers.html">Deep Representation Learning: What and Why!</a> )</figcaption></figure> <p>Thus, the central issue is <strong>representation</strong> — specifically, how much valuable information has been learned from the inherently noisy and sparse datasets. How can we efficiently extract and represent knowledge during the learning process?</p> <p><strong>Professionally but directly, </strong>the illustration of learning and representation leads to the question of <strong>how effectively the learning process can optimize probability, regarding whether language models can understand</strong>. This is why the reasoning ability of a language model is theoretically limited by the original rationality present in the dataset. <strong>It is important to remember that a language model is not a human-like being.</strong> People often mistakenly treat these advanced models as if they possess human qualities. However, they are more accurately described as <strong>complex functions</strong> designed to compress information by maximizing the representation of the balance between every detail and microstructure in the data, regardless of the extent of human knowledge embedded within it.</p> <p>Instead of asking whether these language models can understand things, <strong>we may need to focus more on how effectively they perform specific tasks in a learning-and-inference fashion.</strong> Humans do not design the world, and machine logic is not entirely a human creation.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8cb1e9e0615e" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Are Physicochemical Mechanisms Important to Agent for Science?</title><link href="https://dandelionym.github.io/blog/2024/are-physicochemical-mechanisms-important-to-agent-for-science/" rel="alternate" type="text/html" title="Are Physicochemical Mechanisms Important to Agent for Science?"/><published>2024-07-18T07:10:04+00:00</published><updated>2024-07-18T07:10:04+00:00</updated><id>https://dandelionym.github.io/blog/2024/are-physicochemical-mechanisms-important-to-agent-for-science</id><content type="html" xml:base="https://dandelionym.github.io/blog/2024/are-physicochemical-mechanisms-important-to-agent-for-science/"><![CDATA[<p>People are increasingly turning to the powerful capabilities of current large language models (LLMs) for AI applications in science, driven by their improving accuracy and methodologies. But what exactly is the intelligence?</p> <p>Even without formal theoretical analysis, <strong>I believe true intelligence would be characterized by performing tasks like humans — continuous self-learning and improvement in any environment. </strong>It is similar to human beings, trained on general tasks that already contain fundamental reasoning and memory. It can also perform well in specific tasks in professional disciplines with a little adaption. That’s why the undeniable fact is that for a long time, computer scientists have prioritised general tasks over the diverse abilities that expect an agent to outperform in specific or professional scenarios as well.</p> <p>AI for Science is not a novel area yet more challenging in perspective. Compared to those 99% abilities to support an AI model well-act in general scenarios, that 1%, importantly, for some reason may lead to suffering results in the end, and it’s often the 1% of tasks that lead to significant challenges. These times, people are saying they will solve protein folding or materials designing problems, as they believe the frontiers for sure with 1% are in front of the eye. Notably, there is also an obvious way to let AI behave like humans with specific abilities that are always across disciplines — anybody can tune the model and then directly run queries with prompts upon LLMs, or at a higher level we design some algorithms to let it avoid errors. It is promising that we are all in LLMs for pursuing an AGI.</p> <p>The crux of the issue lies in <strong>understanding how these models can mimic the human ability to comprehend and interact with the physical world</strong>. At the point of comprehension, it is physicochemical mechanisms that define the world, while at the interaction level, rationality shows us perfect cognition.</p> <p>However, it remains a blank area for people to understand the world from a pure physicochemical mechanisms level, instead, they stand on the statistical results and act as a king of Arther. You probably agree that LLMs excel at processing and generating language, but they lack the inherent understanding of the physical world that humans possess. This gap is significant when it comes to scientific applications, where the interplay of physical laws and chemical processes is crucial. For instance, predicting molecular interactions or understanding the behaviour of complex systems requires more than just linguistic proficiency; it demands a deep integration of scientific knowledge and reasoning.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bucdFyhg_waQDSgZ"/><figcaption>Photo by <a href="https://unsplash.com/@cdc?utm_source=medium&amp;utm_medium=referral">CDC</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>To bridge this gap, <strong>it’s essential to develop models that can not only process language but also simulate and reason about physicochemical phenomena.</strong> This requires a multidisciplinary approach, combining advances in AI with insights from physics, chemistry, and biology. By embedding these principles into the training and architecture of LLMs, we can enhance their ability to perform scientific tasks more effectively.</p> <p>Moreover, the integration of sensorimotor capabilities with LLMs can lead to more holistic AI systems. By enabling agents to interact with their environment and gather empirical data, we can foster a more profound understanding of physical processes. This approach mirrors the way humans learn and adapt, through continuous interaction and feedback from the world around them. But anyway, it is the well-defined physicochemical principles that promote the basic understanding of the world, something like causal, or facts with rule-like structures.</p> <p>Stand on the frontier of current AI, I believe we may need more progress towards the essence of AI for Science — that is, leveraging those fundamental principles in the real world to make AI powerful, and assessing their abilities of comprehension in professional knowledge.</p> <p>To go back to the title, I think we even don’t know what the physicochemical mechanisms, simple principles, or great laws are, in the context of AI for Science. But one more unassured guess, if there is one more try to leverage these most advanced technologies, one may realize that the true mechanisms of the world are not all in the sampled data — what machines learned from existing data is only the differences among perspectives of interpreting it with randomness.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=98f9812bc04d" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://pub.aimind.so/are-physicochemical-mechanisms-important-to-agent-for-science-98f9812bc04d">Are Physicochemical Mechanisms Important to Agent for Science?</a> was originally published in <a href="https://pub.aimind.so">AI Mind</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry></feed>