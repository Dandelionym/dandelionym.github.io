<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://dandelionym.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dandelionym.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-17T13:30:24+00:00</updated><id>https://dandelionym.github.io/feed.xml</id><title type="html">Mellen Y. Pu</title><subtitle>Mellen&apos;s personal website. </subtitle><entry><title type="html">Person and Mural</title><link href="https://dandelionym.github.io/blog/2025/person-and-mural/" rel="alternate" type="text/html" title="Person and Mural"/><published>2025-03-16T07:38:02+00:00</published><updated>2025-03-16T07:38:02+00:00</updated><id>https://dandelionym.github.io/blog/2025/person-and-mural</id><content type="html" xml:base="https://dandelionym.github.io/blog/2025/person-and-mural/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uaXpxVpv-5iWEFlB6NqNeQ.jpeg"/></figure> <p><em>The sunset is time’s Adam’s apple — <br/>Each day I watch it lodge at the golden summit of western hills<br/>My shadow and I<br/>Remain silent for ages<br/>The motorway by the building, like an artery,<br/>Frantically ferries the twilight<br/>Mist escapes the mountains’ encirclement, smokestacks in the distant expanse<br/>Suddenly seem incongruous<br/>Everything shall come to pass. My shadow and I<br/>Steel ourselves against one another<br/>Like sunset-stained vine veins sprouting thorns<br/>Nearby, mallards call from the river<br/>The abbey begins its vespers<br/>Let us not lose heart, feigning life<br/>In this purely human realm</em></p> <h4>Author Note:</h4> <p>This is a Chinese poem written by the author, at 2019.</p> <blockquote>© 人与壁画</blockquote> <blockquote>落日是岁月的喉结<br/>每一天我看着它噎在金黄色的西山顶<br/>我与我的影子，都会<br/>沉默很久<br/>楼旁的高速公路像支血管<br/>争分夺秒地搬运着暮色<br/>薄雾脱离了群山的围剿，远处广垣的烟囱<br/>恍然有些突兀<br/>总会了结的。我与影子彼此<br/>相互鼓足了劲<br/>像夕阳晕染过的藤脉长出了刺<br/>不远处，河水里传出水鸭的响声<br/>教堂里开始诵经了<br/>让我们不要失去信心，自己仿佛活着<br/>在这纯粹的人间</blockquote> <p>Yes... Time passes in a blur, days moving faster and faster.</p> <p>Life’s comings and goings always illustrate intersections and parallel lines. Conversing with your own shadow is, without doubt, searching for some inner source or origin.</p> <p>The world created by thought often flows like a babbling river; passing vehicles kick up the dust of four seasons, and along with flower petals and the setting sun, gradually learn to become intoxicated.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9e00839adf71" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">AI for Material Synthesis Science: A Brief Introduction</title><link href="https://dandelionym.github.io/blog/2025/ai-for-material-synthesis-science-a-brief-introduction/" rel="alternate" type="text/html" title="AI for Material Synthesis Science: A Brief Introduction"/><published>2025-02-21T18:31:01+00:00</published><updated>2025-02-21T18:31:01+00:00</updated><id>https://dandelionym.github.io/blog/2025/ai-for-material-synthesis-science-a-brief-introduction</id><content type="html" xml:base="https://dandelionym.github.io/blog/2025/ai-for-material-synthesis-science-a-brief-introduction/"><![CDATA[<p><strong>Q: What can today’s AI do for Science?</strong></p> <p><strong>A: Simply, I say, <em>reading</em>, <em>writting</em>, <em>hypothesizing</em>, and <em>solving</em>.</strong></p> <h3>Introduction</h3> <p>The integration of artificial intelligence, particularly deep learning and language models, has revolutionized the approach to materials synthesis. Deep learning architectures, including convolutional neural networks (CNNs) and graph neural networks (GNNs), have demonstrated remarkable capabilities in <strong>predicting material properties and optimizing synthesis conditions</strong>. These models can process complex structural data and composition-property relationships that were previously challenging to analyze through traditional computational methods.</p> <p>Large language models have emerged as powerful tools for scientific knowledge extraction and synthesis planning. By training on vast corpora of materials science literature, these models can extract synthesis protocols from scientific papers, generate hypotheses about novel material combinations, predict reaction conditions based on precursor materials, and identify patterns in successful synthesis strategies across different material classes.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_i_8EPHId9Ovy-0zKB7Qxg.png"/><figcaption>Particles in material synthesis, generated by Grok from X.</figcaption></figure> <h3>Materials Synthesis: Optimization and Constraints</h3> <p>Materials synthesis represents a multifaceted optimization challenge where scientists must navigate complex property spaces while considering practical constraints. The optimization of material properties encompasses mechanical attributes such as strength and elasticity, electronic characteristics including conductivity and band gap, thermal properties like heat capacity and conductivity, and chemical aspects such as reactivity and stability. These properties often exhibit intricate interdependencies, requiring careful balance during synthesis.</p> <p>The practical aspects of materials synthesis extend beyond theoretical optimization. Scientists must consider the economic viability of their synthesis routes, including raw material costs, energy requirements, and necessary infrastructure investments. Environmental sustainability has become increasingly crucial, driving the development of greener synthesis methods and more efficient resource utilization. The scalability of laboratory procedures to industrial production presents additional challenges, as processes that work well at small scales may encounter unexpected complications during scale-up.</p> <p>Process control represents another critical dimension of materials synthesis. Success often depends on precise manipulation of temperature profiles, pressure conditions, and atmospheric composition. The relationship between precursor ratios, concentrations, and final product properties requires careful understanding and control of reaction kinetics. These parameters must be optimized while maintaining reproducibility and reliability across different synthesis batches.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nr6lVhirx_xcc-fBWuJFVg.png"/><figcaption>AI Robot, generated by Grok 3 from X.</figcaption></figure> <h3>Integration of AI with Experimental Workflows</h3> <p>Modern materials synthesis has evolved into a sophisticated interplay between artificial intelligence and automated experimental platforms. Predictive modeling serves as the cornerstone of this integration, with machine learning models generating insights about optimal synthesis conditions. Bayesian optimization frameworks guide experimental design by suggesting the most informative experiments to perform, while uncertainty quantification helps researchers understand the reliability of model predictions.</p> <p>High-throughput experimentation has become increasingly sophisticated through automation and real-time analysis capabilities. Advanced characterization techniques provide rapid feedback about material properties and structure, enabling dynamic adjustment of synthesis parameters. This creates a powerful feedback loop where experimental results continuously inform and improve predictive models.</p> <p>The integration of theoretical calculations, including density functional theory and molecular dynamics simulations, provides fundamental physical insights that complement empirical observations. Domain expertise captured in structured databases helps contextualize new findings within the broader landscape of materials science. This synthesis of theoretical understanding, experimental data, and domain knowledge creates a robust framework for materials discovery and optimization.</p> <h3>Future Directions</h3> <p><strong>Inverse design. </strong>The future of AI-driven materials synthesis holds exciting possibilities for advancing the field. Inverse design approaches are becoming more sophisticated, enabling researchers to work backward from desired properties to determine viable synthesis routes. This capability dramatically accelerates the development of materials with specific target characteristics.</p> <p><strong>Multi-modal learning. </strong>This learning paradigm represents another frontier, as researchers develop systems capable of integrating diverse data types including spectroscopic measurements, crystallographic information, and microscopy images. This comprehensive approach provides deeper insights into synthesis mechanisms and material properties.</p> <p><strong>Interpretability. </strong>The development of interpretable AI models promises to bridge the gap between predictive power and mechanistic understanding. Rather than treating AI systems as black boxes, researchers are creating models that can provide insights into the fundamental principles governing successful synthesis strategies.</p> <p><strong>Specific-area materials design. </strong>Sustainable materials design has emerged as a critical focus area, with AI tools being leveraged to optimize synthesis conditions for minimal environmental impact while maintaining or improving material performance. This approach aligns with broader societal goals of developing more sustainable technologies and manufacturing processes.</p> <p>The fundamental intersection of AI and materials synthesis, emphasizes both the technological depth and practical constraints that shape the field. The integration of AI tools, particularly deep learning and language models, with traditional materials science approaches continues to accelerate the discovery and optimization of new materials.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0b278f487a86" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://pub.aimind.so/ai-for-material-synthesis-science-a-brief-introduction-0b278f487a86">AI for Material Synthesis Science: A Brief Introduction</a> was originally published in <a href="https://pub.aimind.so">AI Mind</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Can Language Models “Understand” the World? A Misleading Question</title><link href="https://dandelionym.github.io/blog/2024/can-language-models-understand-the-world-a-misleading-question/" rel="alternate" type="text/html" title="Can Language Models “Understand” the World? A Misleading Question"/><published>2024-08-22T05:24:04+00:00</published><updated>2024-08-22T05:24:04+00:00</updated><id>https://dandelionym.github.io/blog/2024/can-language-models-understand-the-world-a-misleading-question</id><content type="html" xml:base="https://dandelionym.github.io/blog/2024/can-language-models-understand-the-world-a-misleading-question/"><![CDATA[<p>The concept of “<strong>understanding</strong>” is inherently subjective, especially when evaluating GPT-series models, which are often considered black boxes. Different people may hold varying perspectives on what constitutes understanding. <strong>While some argue that GPT-4 lacks true comprehension, it’s worth questioning how we measure understanding in humans.</strong> What level of performance or test scores can definitively distinguish between a solid grasp of concepts and mere conjecture?</p> <p>I contend that framing the debate around whether these models “understand” is misleading.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GZ-CbE5OqMdWw9Pt"/><figcaption>Photo by <a href="https://unsplash.com/@benwhitephotography?utm_source=medium&amp;utm_medium=referral">Ben White</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>Let’s start by discussing the <strong>reasoning process</strong>. The most effective approach is to view it from the perspective of general learning. This process is analogous to how humans acquire knowledge through reading, frequent reflection, memorization, and interaction.</p> <p>The reasoning ability of language models can be understood as a reflection of rationality in cognition, or as a data-centric learning problem, rooted in continuous learning from large-scale datasets. Within these human-annotated datasets, cognitive logic and causality from a human perspective are evident. The design of these models aims to enable them to learn both implicit and explicit correlations from token-level representations (commonly understood as word-level, though it’s more nuanced than just individual words). The process of learning natural language tokens is akin to learning images at the pixel level. For instance, just as one can generate images by understanding the distribution of pixels (e.g., mouths are located at the bottom of faces, symmetrical eyes, etc.), in language processing, a “pixel” corresponds to a token. The distribution of a sentence (comprising multiple tokens) conveys information in a way that mirrors human recognition of patterns and characteristics.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YQgo0ViHx9cpgJ2v.png"/><figcaption>Features or representations provided by a good DNN model should encode meaning related to the current task (in this case, classification). With this representation, similar data items are <em>closer</em> to each other (right side); the original non-linear problem is now linearly separable, and therefore easier to solve. (referenced from <a href="https://blog.fastforwardlabs.com/2020/11/15/representation-learning-101-for-software-engineers.html">Deep Representation Learning: What and Why!</a> )</figcaption></figure> <p>Thus, the central issue is <strong>representation</strong> — specifically, how much valuable information has been learned from the inherently noisy and sparse datasets. How can we efficiently extract and represent knowledge during the learning process?</p> <p><strong>Professionally but directly, </strong>the illustration of learning and representation leads to the question of <strong>how effectively the learning process can optimize probability, regarding whether language models can understand</strong>. This is why the reasoning ability of a language model is theoretically limited by the original rationality present in the dataset. <strong>It is important to remember that a language model is not a human-like being.</strong> People often mistakenly treat these advanced models as if they possess human qualities. However, they are more accurately described as <strong>complex functions</strong> designed to compress information by maximizing the representation of the balance between every detail and microstructure in the data, regardless of the extent of human knowledge embedded within it.</p> <p>Instead of asking whether these language models can understand things, <strong>we may need to focus more on how effectively they perform specific tasks in a learning-and-inference fashion.</strong> Humans do not design the world, and machine logic is not entirely a human creation.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8cb1e9e0615e" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Are Physicochemical Mechanisms Important to Agent for Science?</title><link href="https://dandelionym.github.io/blog/2024/are-physicochemical-mechanisms-important-to-agent-for-science/" rel="alternate" type="text/html" title="Are Physicochemical Mechanisms Important to Agent for Science?"/><published>2024-07-18T07:10:04+00:00</published><updated>2024-07-18T07:10:04+00:00</updated><id>https://dandelionym.github.io/blog/2024/are-physicochemical-mechanisms-important-to-agent-for-science</id><content type="html" xml:base="https://dandelionym.github.io/blog/2024/are-physicochemical-mechanisms-important-to-agent-for-science/"><![CDATA[<p>People are increasingly turning to the powerful capabilities of current large language models (LLMs) for AI applications in science, driven by their improving accuracy and methodologies. But what exactly is the intelligence?</p> <p>Even without formal theoretical analysis, <strong>I believe true intelligence would be characterized by performing tasks like humans — continuous self-learning and improvement in any environment. </strong>It is similar to human beings, trained on general tasks that already contain fundamental reasoning and memory. It can also perform well in specific tasks in professional disciplines with a little adaption. That’s why the undeniable fact is that for a long time, computer scientists have prioritised general tasks over the diverse abilities that expect an agent to outperform in specific or professional scenarios as well.</p> <p>AI for Science is not a novel area yet more challenging in perspective. Compared to those 99% abilities to support an AI model well-act in general scenarios, that 1%, importantly, for some reason may lead to suffering results in the end, and it’s often the 1% of tasks that lead to significant challenges. These times, people are saying they will solve protein folding or materials designing problems, as they believe the frontiers for sure with 1% are in front of the eye. Notably, there is also an obvious way to let AI behave like humans with specific abilities that are always across disciplines — anybody can tune the model and then directly run queries with prompts upon LLMs, or at a higher level we design some algorithms to let it avoid errors. It is promising that we are all in LLMs for pursuing an AGI.</p> <p>The crux of the issue lies in <strong>understanding how these models can mimic the human ability to comprehend and interact with the physical world</strong>. At the point of comprehension, it is physicochemical mechanisms that define the world, while at the interaction level, rationality shows us perfect cognition.</p> <p>However, it remains a blank area for people to understand the world from a pure physicochemical mechanisms level, instead, they stand on the statistical results and act as a king of Arther. You probably agree that LLMs excel at processing and generating language, but they lack the inherent understanding of the physical world that humans possess. This gap is significant when it comes to scientific applications, where the interplay of physical laws and chemical processes is crucial. For instance, predicting molecular interactions or understanding the behaviour of complex systems requires more than just linguistic proficiency; it demands a deep integration of scientific knowledge and reasoning.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bucdFyhg_waQDSgZ"/><figcaption>Photo by <a href="https://unsplash.com/@cdc?utm_source=medium&amp;utm_medium=referral">CDC</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>To bridge this gap, <strong>it’s essential to develop models that can not only process language but also simulate and reason about physicochemical phenomena.</strong> This requires a multidisciplinary approach, combining advances in AI with insights from physics, chemistry, and biology. By embedding these principles into the training and architecture of LLMs, we can enhance their ability to perform scientific tasks more effectively.</p> <p>Moreover, the integration of sensorimotor capabilities with LLMs can lead to more holistic AI systems. By enabling agents to interact with their environment and gather empirical data, we can foster a more profound understanding of physical processes. This approach mirrors the way humans learn and adapt, through continuous interaction and feedback from the world around them. But anyway, it is the well-defined physicochemical principles that promote the basic understanding of the world, something like causal, or facts with rule-like structures.</p> <p>Stand on the frontier of current AI, I believe we may need more progress towards the essence of AI for Science — that is, leveraging those fundamental principles in the real world to make AI powerful, and assessing their abilities of comprehension in professional knowledge.</p> <p>To go back to the title, I think we even don’t know what the physicochemical mechanisms, simple principles, or great laws are, in the context of AI for Science. But one more unassured guess, if there is one more try to leverage these most advanced technologies, one may realize that the true mechanisms of the world are not all in the sampled data — what machines learned from existing data is only the differences among perspectives of interpreting it with randomness.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=98f9812bc04d" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://pub.aimind.so/are-physicochemical-mechanisms-important-to-agent-for-science-98f9812bc04d">Are Physicochemical Mechanisms Important to Agent for Science?</a> was originally published in <a href="https://pub.aimind.so">AI Mind</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry></feed>